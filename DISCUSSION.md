Looking at the advocates API route, there's a significant performance concern with how we're handling data fetching and caching. The current implementation pulls the entire advocates table into memory before performing any operations. For a table with hundreds of thousands of records, this creates unnecessary memory pressure and network overhead. If I had more time to spend on this assignment, I would leverage PostgreSQL's capabilities for filtering/ pagination, so we're only fetching the specific records we need for each request rather than loading the entire table into memory. The db is built to handle these operations efficiently with proper indexes - much better than filtering data in our app code. This change would significantly reduce both memory usage and network traffic, since we'd only be transferring the exact records we need.